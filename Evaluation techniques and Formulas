Model Evaluation Metrics

1. Accuracy
-What it means: How often the model gets the prediction correct.
-Formula: Accuracy = Correct Predictions / Total Predictionsl
-Example:
If the model correctly predicts 190 out of 200 test cases: Accuracy = 190/200 = 0.95

2. Precision
-What it means: When the model predicts a certain size (e.g., "S"), how often is it right?
-Formula: Precision = True Positives / True Positives + False Positives
-Example: If the model says "S" 20 times and 18 are correct: Precision= 20/18 =0.9

3. Recall (Sensitivity)
-What it means: Of all the actual "S" sizes, how many did the model correctly find?
-Formula: Recall = True Positives/ True Positives + False Negatives
-Example: If there are 19 actual "S" and model correctly finds 18: Recall= 19/18 ≈0.95

4. F1 Score
-What it means: A balance between precision and recall.
-Formula: F1 Score = 2 × Precision × Recall / Precision + Recall
-Why use it? When you want a single number to judge both precision and recall together.

5. Confusion Matrix
-What it means: A table showing what the model got right and wrong for each class.
-Layout:
         Predicted L	Predicted S	Predicted XL
Actual L	   9	         0	           0
Actual S	   0	         19	           0
Actual XL	   0	          0	          172

​

 


