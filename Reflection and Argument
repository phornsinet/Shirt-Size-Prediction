Reflection and Argument

What This Project Was About?
In this project, I built a machine learning model that predicts T-shirt sizes (S, L, XL) based on a person’s height and weight. I used real data, prepared it properly, and tested different machine learning models to see which one worked best.

What I Learned?
1. Understanding the Data
- I started with a dataset of 1,000 entries containing height, weight, and T-shirt size.
- The data had no missing values, which is great for model training.
- I noticed the dataset was imbalanced most people had size XL (876), but very few had size L (56) or S (68). This is important because some models don’t work well when one class dominates the others.

2. Preprocessing
- I standardized the data to make sure height and weight had equal influence on predictions.
- Standardization means converting values like height and weight into a common scale (mean = 0, std = 1). This helps models like SVM and Logistic Regression perform better.

3. Training and Testing
- I split the data into 80% for training (800 rows) and 20% for testing (200 rows).
- I trained three different models:
  - Random Forest
  - Support Vector Machine (SVM)
  - Logistic Regression

4. Model Evaluation
- I used Accuracy, Classification Reports, and Confusion Matrices to check how well each model performed.
- These evaluation tools helped me see not only the overall accuracy but also how each model performed for each class (S, L, XL).

My Argument/Interpretation
Random Forest Performed Best
- It had 100% accuracy on the test data.
- It correctly predicted all 200 test samples, including the smaller classes (S and L).
- It did not need much tuning, which shows it is powerful and flexible.
- Random Forest works well because it builds many decision trees and uses the vote from all trees to make a prediction. This reduces errors and overfitting.

SVM Was Very Good, But Not Perfect
- It had 98.5% accuracy, which is still excellent.
- It only misclassified a few samples in the L class.
- SVM is a powerful model, but it can struggle when classes are imbalanced. Still, it performed well overall.

Logistic Regression Struggled with Class Imbalance
- It had 95.5% accuracy, which is good overall.
- But it predicted most L T-shirt sizes incorrectly  only 2 out of 9 were correctly identified.
- This is because Logistic Regression is a linear model and doesn't handle complex boundaries or imbalanced data as well.

What This Tells Me?
- Model choice matters a lot. Even with the same data and same training method, different models give very different results.
- Data imbalance affects performance models like Logistic Regression suffer more from this issue.
- Evaluation techniques like classification reports and confusion matrices give more insight than just accuracy.
- Random Forest is a great default choice , especially when you want good performance without complex tuning.

What I Would Do Next?
- Try balancing the dataset using oversampling or undersampling.
- Use GridSearchCV to tune hyperparameters like nestimators or C.
- Visualize decision boundaries to better understand how each model works.
- Try other models like KNN or XGBoost and compare.
- Deploy the model in a simple web app where users can input height and weight to get size recommendations.

Conclution 
This project helped me understand the full cycle of a machine learning problem from data collection to model training, evaluation, and decision making.  
It showed me that choosing the right algorithm, understanding your data, and evaluating carefully are all key steps in building smart AI systems.


